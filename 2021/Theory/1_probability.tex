\RequirePackage{currfile}
\tolerance=5000
\documentclass[10pt, xcolor=x11names,compress,usenames,dvipsnames]{beamer}

\usepackage[english]{babel}

\usepackage[framemethod=TikZ]{mdframed}

\usepackage{amssymb,amsmath,amsfonts,eurosym,geometry,ulem,graphicx,caption,color,setspace,comment,footmisc,caption,pdflscape,subfigure,array,hyperref,upgreek,bbm,xcolor,float,amsthm,amsmath,verbatim,setspace,ulem,textpos,changepage,url,multirow,tikz,color, colortbl,numprint,mathrsfs,cancel,wrapfig,booktabs,threeparttable,ebgaramond,natbib}

\usetikzlibrary{fit,shapes.geometric}

\newcounter{nodemarkers}
\newcommand\circletext[1]{%
    \tikz[overlay,remember picture]
        \node (marker-\arabic{nodemarkers}-a) at (0,1.5ex) {};%
    #1%
    \tikz[overlay,remember picture]
        \node (marker-\arabic{nodemarkers}-b) at (0,0){};%
    \tikz[overlay,remember picture,inner sep=2pt]
        \node[draw,rectangle,red ,fit=(marker-\arabic{nodemarkers}-a.center) (marker-\arabic{nodemarkers}-b.center)] {};%
    \stepcounter{nodemarkers}%
}


\setbeamertemplate{footline}[frame number]


\normalem

\newcommand{\tsout}[1]{\text{\sout{$#1$}}}
\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}c}

\newtheorem{remark}{Remark}
\def\mb{\mathbf}
\def\iid{\mathrm{i.i.d.}}
\def\bs{\boldsymbol}
\def\tbf{\textbf}
\def\t{^{\top}}
\def\E{\mathbbm{E}}
\def\bSig{\bs{\Sigma}}

\newcommand{\mcitet}[1]{\mbox{\citet{#1}}}
\newcommand{\mcitep}[1]{\mbox{\citep{#1}}}
\newcommand{\ind}{\mathbbm{1}}

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vecth}{vech}



\newcommand{\R}{\mathbbm{R}}
\newcommand{\prob}{\mathbbm{P}}
\newcommand{\var}{\text{var}}

\newtheorem{assumption}{Assumption}

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\makeatletter
\newenvironment<>{proofs}[1][\proofname]{%
    \par
    \def\insertproofname{#1\@addpunct{.}}%
    \usebeamertemplate{proof begin}#2}
  {\usebeamertemplate{proof end}}
\makeatother


\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\setbeamertemplate{theorems}[numbered]
% Themes
 \mode<presentation> {
\usetheme{Hannover}
 \usecolortheme{default}
 \setbeamercovered{transparent}
 }

\setbeamercovered{transparent}

\setbeamertemplate{itemize item}{$\triangleright$}
\setbeamertemplate{itemize subitem}{$\diamond$}
\setbeamertemplate{enumerate items}[default]
\setbeamerfont{frametitle}{size=\large}
\PassOptionsToPackage{height=1cm}{beamerouterthemesidebar}
\usepackage{blindtext}

% Title
\title[Probability]{Review of Probability \footnote[frame]{This section is based on \cite{Stock2020}, Chapter 2.}
}
\date[]{\today}


\author[Hao]{Jasmine(Yu) Hao}
\institute[VSE]{Faculty of Business and Economics\\Hong Kong University}


\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section[Probability]{Review of Probability}
\section{Events}
\begin{frame}{Probabilities and Outcomes}
\begin{itemize}
  \item An \tbf{outcomes} is a specific result:
  \begin{itemize}
    \item Coin toss: either $H$ or $T$.
    \item Roll of dice: $1,2 \ldots, 6$.
  \end{itemize}
  \item The \tbf{probability} of an \tbf{outcome} is the proportion of the time that the outcome occurs in the long run.
  \begin{itemize}
    \item Fair coin toss: 50 \% chance of heads.
  \end{itemize}
  \item The \tbf{sample space} is the set of all possible outcomes.
  \begin{itemize}
    \item In a coin flip the sample space is $S = \{H,T\}$.
    \item If two coins are flipped the sample space is $S = \{HH,HT,TH,T T \}$.
  \end{itemize}
  \item An \tbf{event} is a subset of the sample space.
  \begin{itemize}
    \item Roll a die $A = \{1,2\}$.
  \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Probability Function}
\begin{definition}[Probability Function]
A function $\prob$ which assigns a numerical value to events is called a probability function if it satisfies the following Axioms of Probability:
\begin{enumerate}
  \item $\prob(A) \ge 0$.
  \item $\prob(S) = 1$.
  \item If $A_1,A_2,\ldots$ are disjoint then $\prob | \cup_{j=1}^N A_j  | = \sum_{j = 1}^N A_j$.
\end{enumerate}
\end{definition}
\end{frame}

\begin{frame}{Events}
\begin{theorem}[Properties of probability functions]
  For two events, $A$ and $B$,
\begin{enumerate}
  \item $\prob(A^c) = 1 - \prob(A) $.
  \item $\prob(\emptyset) = 0 $.
  \item $\prob(A) \le 1$.
  \item \tbf{Monotone Probability Inequality}: If $ A \subset B$, $\prob(A) \le \prob(B)$.
  \item \tbf{Inclusion-Exclusion Principle}:  $\prob(A \cup B) = \prob(A) +\prob(B) -  \prob(A \cap B) $.
  \item \tbf{Boole's Inequality}: $\prob(A \cup B) \le \prob(A) + \prob (B)$.
  \item \tbf{Bonferroni's Inequality}: $\prob(A \cap B) \ge \prob(A) + \prob (B) - 1 $.
\end{enumerate}
\end{theorem}
Exercise: show \tbf{Bonferroni's Inequality}.(Hint: use the above theorems. )

\end{frame}

\subsection{Joint Events}



\begin{frame}{Conditional Probability}
  \begin{definition}[Conditional Probability]
    If $\prob(B) > 0$, then the \tbf{conditional probability} of $A$ given $B$ is given by \[ \prob(A | B) = \frac{\prob (A \cup B)}{\prob(B)}.\]
  \end{definition}
 $ \prob(B)$ is the \tbf{marginal probability} of event $B$.
 \[ \prob(B) = \prob (A \cup B) +  \prob (A^c \cup B) . \]
\end{frame}

\begin{frame}{Joint Events}
  Take two events $H$ and $C$.
  \begin{itemize}
    \item let be $H$ the event that an individualâ€™s monthly wage exceeds RMB 8000,
    \item let $M$ be the event that the individual has a master's degree.
  \end{itemize}
  \begin{table}[]
    \caption{Joint Distribution}
    \begin{tabular}{|l|l|l|l|}
    \hline
              & Master degree & Non-master degree& Any education \\ \hline
    High wage & \cellcolor[HTML]{9B9B9B}0.19 & \cellcolor[HTML]{9B9B9B}0.12 & 0.31          \\ \hline
    Low wage  & \cellcolor[HTML]{9B9B9B}0.17 & \cellcolor[HTML]{9B9B9B}0.52 & 0.69          \\ \hline
    Any wage  & 0.36                         & 0.64                         & 1.00          \\ \hline
    \end{tabular}
    \end{table}
  \end{frame}

\subsection{Conditional and marginal probability}
  \begin{frame}{Conditional Probability - Example}

  The probability of earning a high wage conditional on high education is \[\begin{split}  & \mathbb{P}( \text{High wage} | \text{Master degree} ) \\ =  & \frac{\prob( \text{High wage} \cup \text{Master degree})}{\prob(\text{High wage} \cup \text{Master degree}) + \prob(\text{Low wage} \cup \text{Master degree})} \\ = & \frac{0.16}{0.36} = 0.53.  \end{split}\]

  Similarly, the probability of earning a high wage conditional on non-master degree is $\mathbb{P}( \text{High wage} | \text{Non-master degree} ) = \frac{0.12}{0.64} = 0.19.$

\end{frame}

\begin{frame}{Independence}
  \begin{definition}[Independence]
    The events $A$ and $B$ are independent if $\mathbb{P} (A \cup B) = \mathbb{P} (A) \mathbb{P} (B)$
  \end{definition}
  \begin{theorem}[Independence]
    If $A$ and $B$ are independent with $\prob(A) > 0$ and $\prob(B) > 0$, then \[ \begin{split}
      \prob(A) = \prob(A | B) , \prob(B) = \prob(B | A).
    \end{split} \]
  \end{theorem}
Some facts:
\begin{itemize}
  \item When events are independent then joint probabilities can be calculated by multiplying individual probabilities.
  \item If $A$ and $B$ are disjoint then they cannot be independent.
\end{itemize}
\end{frame}

% \begin{frame}{Law of total probability * }
% \begin{theorem}[Law of total probability]

% \end{theorem}
% \end{frame}

\begin{frame}{Bayes Rule}
\begin{theorem}[Bayes Rule]
  If $\prob(A) > 0 $ and $\prob(B) > 0 $ then
  \[ \prob(A | B)  =  \frac{\prob(B | A) \prob(A) }{\prob(B | A) \prob(A)  + \prob(B | A^c) \prob(A^c) } . \]
\end{theorem}
\begin{proof}
  Use the Properties of probability functions.
\end{proof}

\end{frame}

\section{Random Variables}
\begin{frame}{Random Variables}
  \begin{definition}[Random variable]
  A random variable is a real-valued outcome; a function from the sample space $S$ to the real line $\R$.
\end{definition}

For example, $X$ is a mapping from the coin flip sample space to the real line, with $T$ mapped to $0$ and $H$ mapped to $1$.

\[ X = \begin{cases}
  1 \quad \text{if H} \\
  0 \quad \text{if T}.
\end{cases} \]
% A coin flip may seem overly simple but the structure is identical to any two-outcome application
Properties of random variables.
\begin{itemize}
  \item The \tbf{expected value} is the long-run average of the random variable.
  \item The \tbf{standard deviation} measures the spread of a probability distribution.
\end{itemize}
\end{frame}

\subsection{Discrete RVs}
\begin{frame}{Discrete Random Variables}

  The set $\mathcal{X}$ is discrete if it has a finite or countably infinite number of elements.
  \begin{definition}[Discrete random variable]
    If there is a discrete set $\mathcal{X}$ such that $\prob(X \in \mathcal{X}) = 1$ then $X$ is a discrete random variable.
  \end{definition}

  The smallest set $\mathcal{X}$ with this property is the support of $X$.

  \begin{definition}[Probability mass function]
    The probability mass function of a random variable is $\pi(x) = \prob(X = x)$, the probability that $X$ equals the value $x$.
  \end{definition}
  \begin{itemize}
    \item The \tbf{probability distribution} of a discrete random variable
    is the list of all possible values of the variable and the probability that each value will occur.
  \end{itemize}

\end{frame}


  \begin{frame}{Expectation}
    For a dsicrete variable $X$ with the support of $\mathcal{X}$, the expctation is computed as
    \[ \E(X) = \sum_{x \in \mathcal{X}} \pi(x) x. \]
    \begin{itemize}
      \item $X = 1$ with the probability of $p$ and $X = 0$ with probability $1 - p$. The expected value is $\E(X) = 1 * p + 0 * (1-p) = p$.
    \end{itemize}

    The expectation of the function of $X$, $g(X)$ is computed as \[ \E(g(X)) = \sum_{x \in \mathcal{X}} \pi(x) g(x). \]
\end{frame}

\begin{frame}{St. Petersburg Paradox}
  Note that the expectation of a distribution does not neccesarily exist.
  \begin{itemize}
    \item  A single player in which a fair coin is tossed at each stage.

    \item The initial stake begins at \$ 2 and is doubled every time heads appears.

    \item The first time a tail appears, the game ends and the player wins whatever is in the pot.

    \item How much would a rational agent pay to get in the bet.

  \end{itemize}
  $X$ has the support of $\mathcal{X} = \{ 2^k : k = 1,\ldots \}$.
  The probability distribution is defined by $\pi(2^k) = 2^{- k }$.
  Then the expectation of $X$ is
  \[ \E(x) = \sum_{k=1}^{\infty} 2^{k} \pi(2^k) = 1 + 1 \ldots = \infty . \]
\end{frame}

\begin{frame}{Cumulative distribution function}
  The \tbf{cumulative probability distribution($CDF$)} is the probability that the random variable is less than or equal to a particular value,
  \[ F(x) = \mathbb{P}(X \le x) , \]
  where the probability event is $X \le x$.
  \begin{theorem}[Properties of a CDF]
   If $F(x)$ is a distribution function, then
   \begin{enumerate}
     \item $F(x)$ is non-decreasing.
     \item $\lim_{x \to -\infty} F(x) = 0$.
     \item $\lim_{x \to \infty} F(x) = 1$.
     \item $F(x)$ is right-continuous, $ \lim_{x \downarrow x_0} F(x) = F(x_0)$.
   \end{enumerate}
  \end{theorem}
\end{frame}

\begin{frame}{Example-Probability mass function}
  Some examples for discrete variables.

  \begin{itemize}
    \item For a fair dice toss, the support is $\mathcal{X} = \{1,2\ldots,6\}$ with the probability mass function is $\pi(x) = \frac{1}{6}$ for $x \in \mathcal{X}$.
    \item An example of infinite countable random variable is the Poisson distribution, the probability mass function is $\pi(x) = \frac{e^{-1}}{x!}, x = 0,1,\ldots$.
    \end{itemize}
    \includegraphics[width=\textwidth]{../figure/W1_1.png}
  \end{frame}

\begin{frame}{Example-Probability function}
  Some examples for discrete variables.

  \begin{itemize}
    \item For a fair dice toss, the support is $\mathcal{X} = \{1,2\ldots,6\}$ with the probability mass function is $\pi(x) = \frac{1}{6}$ for $x \in \mathcal{X}$.
    \item An example of infinite countable random variable is the Poisson distribution, the probability mass function is $\pi(x) = \frac{e^{-1}}{x!}, x = 0,1,\ldots$.
    \end{itemize}
    \includegraphics[width=\textwidth]{../figure/W1_2.png}
    \end{frame}


\subsection{Continuous RVs}
\begin{frame}{Continuous Random Variables}
  \begin{itemize}
  \item The \tbf{probability density function($p.d.f$)} area under the probability density function between any two points is the probability that the random variable falls between those two points.

  \begin{itemize}
    \item the probability for a continous variable to take any value is 0.
    \item definition is different from discrete random variables.
  \end{itemize}
  \item When $F(x)$ is differentiable, the density function is $f(x) = \frac{d F(x)}{d x}$.
\end{itemize}
\begin{theorem}[Properties of density function]
  A function $f(x)$ is a density function if and only if
  \begin{itemize}
    \item $f(x) \ge 0 \forall x $.
    \item $\int_{0}^\infty f(x) d x = 1$.
  \end{itemize}
\end{theorem}
\end{frame}

\begin{frame}{Example - Continuous Variables}
  \begin{itemize}
    \item Uniform distribution. The CDF is $F(x) = \begin{cases}
      0 \quad \text{ if} \quad x < 0\\
      x \quad \text{ if} \quad 0 \le  x \le 1\\
      1 \quad \text{ if} \quad x > 1
    \end{cases}$.
    The PDF is $f(x) = \begin{cases}
      1 \quad \text{ if} \quad \quad 0 \le  x \le 1\\
      0 \quad \text{ elsewhere}
    \end{cases}$.
    \item Exponential distribution. The CDF is $F(x) = \begin{cases}
      0 \quad \text{ if} \quad x < 0\\
      1 - \exp(-x) \quad \text{ if}  x \ge 0
    \end{cases}$. The PDF is $f(x) = \exp(-x), x \ge 0$.
  \end{itemize}
\end{frame}

\begin{frame}{Expectation}
  If $X$ is a continuous random variable with the density function $f(x)$, its expectation is defined as
  \[\E(X) = \int_{-\infty}^{\infty} x f(x) dx\] when the integral is convergent.

  The expectation of the function of $X$, $g(X)$ is computed as \[ \E(g(X)) = \sum_{x \in \mathcal{X}} \int_{-\infty}^{\infty} g(x) f(x) dx. \]

  Some examples:
  \begin{itemize}
    \item $f(x) = 1 $ if $0 \le x \le 1$, $\E(X) = \int_0^1 x f(x) = 0.5$.
    \item $f(x) = \exp(-x)$ if $x \ge 0$, $\E(X) = \int_0^\infty x \exp(-x) d x = 1$(integration by part).
  \end{itemize}
\end{frame}

\subsection{Moments}
\begin{frame}{Mean, variance and Higher Moment}
  Suppose $X$ is a random variable(either discrete or continous).
  \begin{itemize}
    \item The \tbf{mean} of $X$ is $\mu = \E(X)$.
    \item The \tbf{variance} of $X$ is $\sigma^2 = var (X) = \E( (X - \E(X))^2 )$.
  \begin{itemize}
    \item The \tbf{standard deviation} of $X$ is the positive root of the variance, $\sigma = \sqrt{\sigma^2}$.
  \end{itemize}
  \item The \tbf{$m-th$ moment} of $X$ is $\mu'_m = \E(X^m)$ and the \tbf{$m-th$ central moment} of $X$ is $\mu_m = \E( (X - \E(X))^m)$.
  \begin{itemize}
    \item The \tbf{skewness} of $X$ is defined as $skewness = \frac{\E( (X - \E(X))^3 )}{\sigma^3}$. If the distribution is symmetric, the skewness is 0.
    \item The \tbf{kurtosis} of $X$ is defined as $skewness = \frac{\E( (X - \E(X))^4 )}{\sigma^4}$.
  \end{itemize}
\end{itemize}
\end{frame}


\section{Multivariate Distributions}
\begin{frame}{Bivariate random variables}
  A pair of \tbf{bivariate random variables} is a pair of numerical outcomes; a function from the sample space to $\R^2$.

  A pair of bivariate random variables are typically represented by a pair of uppercase Latin characters such as $(X,Y)$.
  Specific values will be written by a pair of lower case characters, e.g. $(x, y)$.

  \begin{figure}
    \caption{Tossing two coins}
    \includegraphics[]{../figure/W1_3.png}
  \end{figure}
\end{frame}

\begin{frame}{Joint distribution functions}
The \tbf{joint distribution function(Joint CDF)} of $(X,Y)$ is defined as $F(x, y) = \prob(X \le x, Y \le y) = \prob(\{X \le x\} \cap \{Y \le y\}).$

\begin{itemize}
  \item A pair of random variables is \tbf{discrete} if there is a discrete set $ \mathscr(P) \in \R^2 $ such that $\prob( (X,Y) \in \mathscr{P} ) = 1$.
  \begin{itemize}
    \item  The set $\mathcal{P}$ is the support of $(X,Y)$ and consists of a set of points in $\R^2$.
    \item The \tbf{joint probability mass function} is defined as $p(x,y) = \prob(X = x, Y = y)$.
  \end{itemize}
  \item The pair $(X,Y)$ has a continuous distribution if the joint distribution function $F(x, y)$ is \tbf{continuous} in $(x, y)$.
  \begin{itemize}
    \item When $F(x,y)$ is continuous and differentiable its \tbf{joint density (joint PDF)} $f (x, y)$ equals $f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y} .$
  \end{itemize}

\end{itemize}
\end{frame}

\begin{frame}{Bivariate expectation}
  The \tbf{expected value} of real-valued $g(X,Y)$ is
  \[ \E(g(X,Y)) = \sum_{(x,y) \in \R^2, \pi(x,y) > 0} g(x,y) \pi(x,y), \]
  for discrete variables and
  \[ \E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) d x d y. \]
\end{frame}

\subsection{Marginal and conditional distributions}
\begin{frame}{Marginal distribution}

The \tbf{marginal distribution(marginal CDF)} of $X$ is \[ F_X(x) = \prob (X \le x) = \prob (X \le x, Y \le \infty) = \lim_{y \to \infty} F(x,y). \]
\begin{itemize}
  \item In the continuous case, \[F_X(x) = \lim_{y \to \infty} \int_{-\infty}^{y} \int_{-\infty}^{x} f(u,v) d u d v = \int_{-\infty}^{\infty} \int_{-\infty}^{x} f(u,v) d u d v .\]
  \item The \tbf{marginal densities(marginal PDF)} of $X$ is the derivative of the marginal CDF of $X$, \[ f_X(x) = \frac{d}{d x}  F_X(x) = \frac{d}{d x}\int_{-\infty}^{\infty} \int_{-\infty}^{x} f(u,v) d u d v =  \int_{-\infty}^{\infty} f(x,y) dy .\]
  \item Similarly, the marginal PDF of $Y$ is  \[ f_Y(y) = \int_{-\infty}^{\infty} f(x,y) dx .\]
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Conditional distributions}
The conditional cumulative distributions:
  \begin{itemize}
    \item  The \tbf{conditional distribution function} of $Y$ given $X = x $  is \[ F_{Y|X}(y|x) = \prob(Y \le y | X = x)  \] for any x such that $\prob (X = x) > 0$ ,  If $X$ has a discrete distribution.
    \item For continous $X,Y$, the \tbf{conditional distribution} of $Y$ given $X = x$ is  \[ F_{Y|X}(y|x) = \lim_{\epsilon \downarrow 0} \prob(Y \le y | x - \epsilon \le X \le x + \epsilon).\]
    If $F(x,y)$ is differentiable w.r.t $x$ and $f_X(x) > 0$, \[F_{Y|X}(y|x) = \frac{\frac{\partial}{\partial x}F(x,y)}{f_X(x)}.\]
  \end{itemize}
  The conditional density:
  \begin{itemize}
    \item For continous variable $(X,Y)$, the conditional density function (conditional PDF) is defined by $f_{Y|X}(y|x) = \frac{d}{d y} f_{Y|X}(y|x).$
  \end{itemize}
\end{frame}

\subsection{Independence}
\begin{frame}[allowframebreaks]{Independence}
  \begin{itemize}
    \item  Recall that two events $A$ and $B$ are independent if the probability that they both occur equals the product of their probabilities, thus $\prob(A \cap B) = \prob(A) \prob(B)$.
    \item Consider the events $A =\{ X \le x\} $ and $B = \{ Y \le y \}$.
    \item The probability that they both occur is $\prob (A \cap B) = \prob(X \le x, Y\le y) = F(x,y)$.
    \item If $F(x,y) = F_X(x) F_Y(y)$ then $\prob(A \cap B) = \prob(A) \prob(B)$.
    \item The random variables $X$ and $Y$ are \tbf{statistically independent} if for all $x,y$, $F(x,y) = F_X(x) F_Y(y)$.
    \item The discrete random variables $X$ and $Y$ are \tbf{statistically independent} if for all $x,y$, $\pi(x,y) = \pi_X(x) \pi_Y(y)$.
    \item If $X,Y$ have differentiable density function, $X,Y$ are statistically independent if $f(x,y) = f_X(x) f_Y(y)$.
  \end{itemize}

\begin{theorem}
  If $X,Y$ are independent and continuously distributed,
  then \[\begin{split}
    f_{Y|X}(y|x) = f(y), \\
    f_{X|Y}(x|y) = f(x).
  \end{split} \]
\end{theorem}

\begin{theorem}(Bayes Theorem for Densities)
  \[ f_{Y|X}(y|x) = \frac{f_{X|Y}(x|y) f_{Y}(y)}{f_X(x)} = \frac{f_{X|Y}(x|y) f_{Y}(y)}{ \int_{-\infty}^\infty f(x,y) dy} .\]
\end{theorem}
\pagebreak
\begin{theorem}
  If $X$ and $Y$ are independent then for any functions, $g : \R \to \R$ and $h : \R \to \R$ such that $\E|g(X)| < \infty$ and $\E|h(Y)| < \infty$, then \[ \E( g(X) h(G) ) = \E_X (g(X)) \E_Y( h(Y)).\]
\end{theorem}
\end{frame}

\begin{frame}[allowframebreaks]{Covariance and correlation}
  \begin{itemize}
    \item If $X$ and $Y$ have finite variances, the \tbf{covariance} between $X$ and $Y$ is \[cov(X,Y) = \E (X - \E(X))(Y - \E(Y) )) = \E(XY) - \E(X) \E(Y). \]
    \item The \tbf{correlation} between $X$ and $Y$ is \[ corr(X,Y) = \frac{cov(X,Y)}{\sqrt{ \var(X) \var(Y) }}. \]
    \pagebreak
    \item If $X$ and $Y$ are independent with finite variances, then $X$ and $Y$ are uncorrelated.
    \begin{itemize}
      \item The reverse is not true. For example, suppose that $X \sim U[-1,1]$.
      Since it is symmetrically distributed about 0 we see that $\E[X] = 0$ and $\E[X^3] = 0$.
      Set $Y = X^2$. Then $cov(X,Y) = \E[X^3] - \E[X^2] \E[X] = 0.$ Thus $X$ and $Y$ are uncorrelated yet are fully dependent!
    \end{itemize}
    \item If $X$ and $Y$ have finite variances, $\var(X,Y) = \var(X) + \var(Y) + 2 cov(X,Y)$.
    \item If $X$ and $Y$ are independent, $\var(X,Y) = \var(X) + \var(Y) $.
  \end{itemize}
\end{frame}

\subsection{Conditional Expectation}
\begin{frame}{Conditional Expectation}
  Just as the expectation is the central tendency of a distribution, the conditional expectation is the central tendency of a conditional distribution.

  The \tbf{conditional expectation(conditional mean)} of $Y$ given $X = x$ is the expected value of the conditional distribution $F_{Y|X}(y|x)$ and is written as $\E(Y | X= x)$.
  \begin{itemize}
    \item For discrete random variables, it is defined as \[ \E(Y | X= x) = \frac{\sum_{y} y \pi(x,y)}{\pi_X(x)}. \]
    \item For continuous random variables, it is defined as \[\E[Y|X=x] = \frac{\int_y y f(x,y)}{f_X(x)}.\]
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Law of Iterated Expectations}
  \begin{itemize}
    \item The function $\E[Y|X=x]$ is not random, it is a feature of the distribution function. But it is useful to treat the conditional expectation as a random variable.
    \item Consider $m(X) = \E[Y|X]$ a transformation of $X$.
    \item We can take expectation with respect to $m(X)$
  \end{itemize}

  \begin{theorem}[Law of Iterated Expectations(LIE)]
    If $\E[Y] < \infty$, then $\E[\E[Y|X]] = \E[Y]$.
  \end{theorem}
  \pagebreak

  \begin{proofs}
   For discrete random variables $X,Y$, \[ \begin{split}
     \E[\E[Y|X]] & = \sum_{x} \pi_X(x) \E[Y|X=x] \\
     & = \sum_{x} \pi_X(x) \frac{\sum_{y} y \pi(x,y)}{\pi_X(x)} \\
     & = \sum_{x} \sum_{y} y \\
     & = \E[Y].
   \end{split} \]
  \end{proofs}

  \pagebreak

  \begin{proof}[Proof.(Cont.)]
   For continuous random variables $X,Y$, \[\begin{split}
   \E[\E[Y|X]] & = \int_{-\infty}^{\infty} f_X(x) \E[Y|X=x] d x\\
   & = \int_{-\infty}^{\infty} f_X(x) \frac{\int_{-\infty}^{\infty} y f(x,y) f y}{\pi_X(x)} d x \\
   & = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} y ~ dy ~ dx \\
   & = \E[Y].
  \end{split}
   \]
  \end{proof}
\end{frame}

\begin{frame}{Conditional Variance}
  The \tbf{conditional variance} of $Y$ given $X = x$ is the variance of the condition distribution $F_{Y|X}(y | x)$ and is written as $\var(Y|X = x)$ or $\sigma^2_Y(x)$.
  It equals \[ \var(Y|X=x) = \E[ (Y - m(x))^2 | X= x ],\]
  where $m(x) = \E[Y | X = x]$.

  Note that  $\var(Y) = \E[\var(Y|X)] + \var(\E[Y|X]) $, where
  \begin{itemize}
    \item
    $\E[\var(Y|X)]$ is the \tbf{within group variance}.
    \item $\var(\E[Y|X])$ is the \tbf{across group variance}.
  \end{itemize}
\end{frame}


\section{Normal and related distributions}
\begin{frame}{Univariate Normal}
  \begin{definition}[Standard Normal Dist.]
    A random variable $Z$ has the \tbf{standard normal distribution}, write $Z \sim N(0,1)$, if it has the density \[ \phi(x) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}), x \in \R. \]
  \end{definition}

  Note the standard normal density is typically written as $\phi(x)$. The CDF does not have closed form but is written as $\Phi(x)$.

  If $X \sim N(\mu, \sigma^2)$ and $\sigma > 0$ then $X$ has the density \[ f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{ (x-\mu)^2}{2 \sigma^2}), x \in \R. \]
\end{frame}

\begin{frame}[allowframebreaks]{Multivariate Normal}
  Let $Z_1,\ldots, Z_m$ be i.i.d $N(0,1)$.
  The joint density is the product of the marginal densities:
  \[\begin{split}
   f(x_1,\ldots,x_m) & = f(x_1) \ldots f(x_m) \\
   & = \frac{1}{(2\pi)^{m/2}} \exp \left( - \frac{1}{2} \sum_{i=1}^m x_i^2 \right).
  \end{split}\]
  \pagebreak

  Let $\mb Z = [Z_1,\ldots,Z_m]\t$ be an m-component random vector following standard normal distribution $\mb Z \sim N(0,\mb I_m)$ and $\mb X = \mb \mu + \mb B \mb Z$ for $q \times m$ matrix $\mb B$, then $\mb X$ has the \tbf{multivariate normal distribution}, written as $\mb X \sim N(\mb \mu, \Sigma)$, where $\Sigma = \mb B \mb B\t$.

  The PDF of $\mb X$ is given by \[ f(\mb x) = \frac{1}{(2\pi)^{q/2}(\det \Sigma)^{1/2} } \exp \left( - \frac{(x - \mu)\t \Sigma^{-1} (x - \mu) }{2} \right) .\]
  \pagebreak


  Properties of multivariate normal distributions.
  \begin{enumerate}
    \item Any linear combination of $X_1,\ldots, X_m$ is normally distributed.
    \item The marginal distribution of each random variable is normal.
    \item If the covariance of $X_1$ and $X_2$ is 0, then $X_1$ and $X_2$ are independent. The reverse is true.
    \item If $X_1$ and $X_2$ are normally distributed with the joint density of $f(x_1,x_2)$, then the marginal distribution of $X_1$ given $X_2$ is a linear function of $X_2$: $\E[X_1|X_2 = x_2] = a + b x_2$.
  \end{enumerate}
\end{frame}

\begin{frame}{$\chi$-squared, t and F distribution}
  The \tbf{chi-squared distribution} is the distribution of the sum of $m$ squared independent standard normal distributed variables.
  \begin{itemize}
    \item Let $\mb Z \sim N(0,\mb I_m)$ be multivariate standard normal, then $\mb Z\t \mb Z \sim \chi_m^2$.
    \item If $mb X \sim N(0,\Sigma)$ with $\Sigma$ positive definite, then $\mb X\t \Sigma^{-1} \mb X$.
    \item Let $Q_m \sim chi_m^2$ and $Q_r \sim chi_r^2$ be independent. Then $\frac{ Q_m / m}{Q_r / r} \sim F_{m,r}$.
    \item Let $Z \sim N(0, 1)$ and $Q_m \sim chi_m^2$ be independent, then $\frac{Z}{ \sqrt{Q_r / r}}$ follows the $t$-distribution with $m$ degree of freedom, $t_m$.
  \end{itemize}
\end{frame}

\section{Sampling}

\begin{frame}[allowframebreaks]{Sampling}
  Almost all the statistical and econometric procedures used in this text
involve averages or weighted averages of a sample of data.

\begin{itemize}
\item
  Characterizing the distributions of sample averages therefore is an
  essential step toward understanding the performance of econometric
  procedures.
\item
  The act of random sampling---that is, randomly drawing a sample from a
  larger population---has the effect of making the sample average itself
  a random variable.
\item
  Because the sample average is a random variable, it has a probability
  distribution, which is called its \textbf{sampling distribution}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \tbf{Simple random sampling}. A commuting student aspires to be a
  statistician and decides to record her commuting times on various
  days. She selects these days at random from the school year, and her
  daily commuting time has the cumulative distribution function. Because
  these days were selected at random, knowing the value of the commuting
  time on one of these randomly selected days provides no information
  about the commuting time on another of the days; that is, because the
  days were selected at random, the values of the commuting time on the
  different days are independently distributed random variables.
\item
  \tbf{i.i.d. draws}. Because \(Y_1,\ldots,Y_n\) are randomly drawn from the
  same population, the marginal distribution of \(Y_i\) is the same for
  each \(i=1,\ldots,n\); this marginal distribution is the distribution
  of \(Y\) in the population being sampled. When \(Y_i\) has the same
  marginal distribution for \(i=1,\ldots,n\), then \(Y_1,\ldots,Y_n\)
  are said to be identically distributed. Under simple random sampling,
  knowing the value of \(Y_1\) provides no information about \(Y_2\),
\end{enumerate}

\end{frame}

\begin{frame}[allowframebreaks]{i.i.d}
  \begin{definition}[independent and identically distributed(i.i.d)]
    The collection of random vectors $\{ X_1,\ldots,X_n \}$ are \tbf{independent and identically distributed(i.i.d)} if they are mutually independent with identical marginal distributions.
  \end{definition}

  \begin{itemize}
    \item  A collection of random vectors $\{ X_1,\ldots,X_n \}$ is a \tbf{random sample} from the population $F$ if $X_i$ are i.i.d with distribution $F$.
    \item The distribution $F$ is called the \tbf{population distribution}. We refer to the distribution as the \tbf{data generating process(DGP)}.
    \item The \tbf{sample size} $n$ is the number of individuals in the sample.
  \end{itemize}
  \begin{figure}
    \caption{Sampling and Inference}
    \includegraphics[width=\linewidth]{../figure/W1_4.png}
  \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{Identification }
  A critical and important issue in structural econometric modeling is \tbf{identification}, meaning that a  parameter is uniquely determined by the distribution of the observed variables. \footnote[frame]{For further reading, see \cite{Hansen2021a} Sec. 4.26.}
  \begin{itemize}
    \item  It is relatively straightforward in the context of the unconditional and conditional expectation, but it is worthwhile to introduce and explore the concept at this point for clarity.
    \item Let $F$ denote a probability distribution, for example the distribution of the pair $(Y ,X)$. Let $F$ be a collection of distributions. Let $\mu$ be a parameter of interest (for example, the mean $E[Y]$).
  \end{itemize}
  \begin{definition}
    A parameter $\theta \in \R^k$ is identified on $F$ if for all $F \in \mathcal{F}$ there is a unique value of $\theta$.
  \end{definition}
  \pagebreak
  \begin{itemize}
    \item Let $(Y ,X)$ have a joint distribution. If $E[Y] < \infty$ the conditional expectation $m (X) = \E[Y | X]$ is identified almost everywhere.
  \end{itemize}
\end{frame}

\section{Large-Sample Approximations to Sampling Distributions}
\begin{frame}[allowframebreaks]{}
  
Sampling distributions play a central role in the development of
statistical and econometric procedures, so it is important to know, in a
mathematical sense, what the sampling distribution of \(Y\) is. \\
There are two approaches to characterizing sampling distributions: an
``exact'' approach and an ``approximate'' approach.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The exact approach entails deriving a formula for the sampling
  distribution that holds exactly for any value of \(n\). The sampling
  distribution that exactly describes the distribution of \(Y\) for any
  n is called the \textbf{exact distribution} or \textbf{finite-sample
  distribution} of \(Y\). For example, if \(Y\) is normally distributed
  and \(Y_1, \ldots , Y_n\) are i.i.d., then the exact distribution of
  \(Y\) is normal with mean \(\mu_Y\) and variance \(\sigma_Y^2/n\).
  Unfortunately, if the distribution of \(Y\) is not normal, then in
  general the exact sampling distribution of \(Y\) is very complicated
  and depends on the distribution of \(Y\).
\item
  The approximate approach uses approximations to the sampling
  distribution that rely on the sample size being large. The
  large-sample approximation to the sampling distribution is often
  called the asymptotic distribution---``asymptotic'' because the
  approximations become exact in the limit that \(n \to \infty\) . As
  we see in this section, these approximations can be very accurate even
  if the sample size is only \(n = 30\) observations. Because sample
  sizes used in practice in econometrics typically number in the
  hundreds or thousands, these asymptotic distributions can be counted
  on to provide very good approximations to the exact sampling
  distribution.
\end{enumerate}

This section presents the two key tools used to approximate sampling
distributions when the sample size is large: \textbf{the law of large
numbers} and the \textbf{central limit theorem}.

\begin{itemize}
\item
  The law of large numbers says that when the sample size is large,
  \(\bar Y\) will be close to \(\mu_Y\) with very high probability.
\item
  The central limit theorem says that when the sample size is large, the
  sampling distribution of the standardized sample average,
  \((\bar Y - \mu_Y) / \sigma_Y\), is approximately normal.
\end{itemize}
\end{frame}
\begin{frame}{Convergence}
  \begin{itemize}
    \item  A sequence of random variables $Z_n \in \R$ \tbf{converges in probability} to $c$ as $n \to \infty$, denoted by $Z_n \to_p c$ or $plim_{n \to \infty} Z_n = c$, if for all $\delta > 0$, \[ \lim_{n \to \infty} \prob ( |Z_n -c| \le \delta) = 1 .\]
    \item Let $Z_n$ be a sequence of random variables or vectors with distribution $G_n(u) = \prob (Z_n \le u)$. We say that $Z_n$ \tbf{converges in distribution} to $Z$ as $n \to \infty$, denoted with $Z_n \to_d Z$, if for all $u$ at which $G(u) = \prob ( Z \le u)$ is continous, $G_n(u) \to G(u)$ as $n \to \infty$.
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Law of Large Numbers}
  \begin{theorem}[Weak Law of Large Numbers(WLLN)]
    If $X_i$ are independent and identically distributed and $\E(X) < \infty$, then as $n \to \infty$, \[ \bar{X_n} = \frac{1}{n} \sum_{i=1}^n X_i \to_p \E(X).\]
  \end{theorem}
  An estimator $\hat \theta$ of a parameter $\theta$ is \tbf{consistent} if $\hat \theta \to_p \theta$ as $n \to \infty$.

  Counter examples:
  \begin{itemize}
    \item $X_i = Z + U_i$, $Z$ is common component and $\E[U] = 0$, $\bar X_i \to_p Z$ but not the sample mean.
    \item Suppose $\var(X_i) = 1$ for $i \le n/2$ and $\var(X_i) = n$ for $i > n/2$. $\var(\bar X_n) \to 1/2$, $\bar X_n$ does not converge in probability.
  \end{itemize}
  \end{frame}

  \begin{frame}{Continuous Mapping Theorem}
  \begin{theorem}[Continuous Mapping Theorem]
    If $Z_n \to_p c$ as $n \to \infty$ and $h(\cdot)$ is continuous at $c$ then $h(Z_n) \to_p h(c)$ as $n \to \infty$.
  \end{theorem}
\end{frame}

\begin{frame}[allowframebreaks]{Central Limit Theorem}
  \begin{theorem}[Central Limit Theorem(CLT)]
    If $X_i$ are i.i.d. and $\E(X^2) < \infty$ then as $n \to \infty$
    \[ \sqrt{n}(\bar X_n \to \mu) \to_d N(0,\sigma^2) ,\]
    where $\mu = \E(X)$ and $\sigma^2 = \E[(X - \mu)^2]$.
  \end{theorem}

  \begin{theorem}[Slutsky's Theorem]
    If $Z_n \to_d Z$ and $c_n \to_p c$ as $n \to \infty$, then
    \begin{enumerate}
      \item $Z_n + c_n \to_d  Z + c$
      \item $Z_n c_n \to_d Z_c$
      \item $\frac{Z_n}{c_n} \to_d \frac{Z}{c}$ if $c \neq 0$.
    \end{enumerate}
  \end{theorem}

  \pagebreak

  \begin{theorem}[Delta Method]
    If $\sqrt{n}(\hat \theta - \theta) \to_d \xi$ and $h(u)$ is continuously differentiable in a neighborhood of $\theta$ then as $n \to \infty$
    \[ \sqrt{n} (h(\hat \theta )- h(\theta)) \to_d \mb H' \xi \]
    where $\mb H(u) = \frac{\partial}{\partial u} h(u)\t $ and $\mb H = \mb H(\theta)$.
  \end{theorem}
\end{frame}
  \begin{frame}[allowframebreaks,noframenumbering]
  \frametitle{References}
  \bibliographystyle{apalike}
\bibliography{library}
\end{frame}

\end{document}
