\documentclass[11pt]{article}
\usepackage{amssymb,amsmath,amsfonts,eurosym,ulem,graphicx,caption,color,setspace,sectsty,comment,footmisc,caption,natbib,pdflscape,subfigure,array,hyperref,upgreek}
\usepackage{bbm}
\usepackage{apalike}
\usepackage{appendix}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
  language=R,  %代码语言使用的是matlab
  frame=shadowbox, %把代码用带有阴影的框圈起来
  rulesepcolor=\color{black!20},%代码块边框为淡青色
  keywordstyle=\color{black!90}\bfseries,
  commentstyle=\color{black!70}\textit,    % 设置代码注释的颜色
  showstringspaces=false,%不显示代码字符串中间的空格标记
  numbers=right, % 显示行号
  numberstyle=\tiny,    % 行号字体
  stringstyle=\ttfamily, % 代码字符串的特殊格式
  breaklines=true, %对过长的代码自动换行
  extendedchars=false,
  texcl=true}

% \usepackage{palatino}


%\onehalfspacing
\usepackage[a4paper, margin=0.9in]{geometry}
\sectionfont{\fontsize{12}{13}\selectfont}
\subsectionfont{\fontsize{11}{12}\selectfont}
\renewcommand{\baselinestretch}{1.1}

\newcommand{\Blue}{\color{blue}}

\def\mb{\mathbf}
\def\iid{\mathrm{i.i.d. }}
\def\bs{\boldsymbol}
\def\tbf{\textbf}
\def\t{^{\top}}
\def\bSig{\bs{\Sigma}}
\newcommand{\mcitet}[1]{\mbox{\citet{#1}}}
\newcommand{\mcitep}[1]{\mbox{\citep{#1}}}

\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\vecth}{vech}


\onehalfspacing
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}


\newcommand{\R}{\mathbb{R}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\prob}{\mathbbm{P}}
\newcommand{\var}{\text{var}}

\newtheorem{assumption}{Assumption}

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{}\renewcommand{\headrulewidth}{0pt} % Blank out the default header
\fancyfoot[L]{} % Custom footer text
\fancyfoot[C]{} % Custom footer text
\fancyfoot[R]{\thepage} % Custom footer text

\begin{document}
%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip
\begin{minipage}{0.9\textwidth}
\centering
\large
\textsc{Applied Econometrics, 2021}\\
\textsc{Assignment} 2\\
\normalsize

\textsc{Due: September 21th, 2021}
\end{minipage}
\medskip \hrule
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------


\noindent \textbf{Instructions.} Do your best to make your arguments
rigorous. You may discuss this problem set with your classmates and consult
any books or notes, but write out the answers on your own using your own
words and show your derivation so that your understanding is transparent
from the answers.  \bigskip




\begin{enumerate}
	\item Suppose that we have the following regression model:
	\begin{eqnarray}
	\label{reg}
	Y_i = \beta_0 + \beta_1 X_i + u_i,
	\end{eqnarray}
	where we assume that $\mathbf{E}[u_i|X_i] = 0$, $\mathbf{E}[u_i^2|X_i] = \sigma^2$, and $Y_i$ and $X_i$ are observed random variables but $u_i$ is an unobserved random variable. We assume that $(Y_i,X_i)$'s are i.i.d. across $i$'s, and that all the random variables have finite variance. Let $\hat \beta_0$ and $\hat \beta_1$ be the least squares estimators of $\beta_0$ and $\beta_1$. We construct the following residual:
	\begin{eqnarray*}
		\hat u_i = Y_i - (\hat \beta_0 + \hat \beta_1 X_i),
	\end{eqnarray*}
	and construct the following estimator of $\sigma^2$,
	\begin{eqnarray*}
		\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n \hat u_i^2.
	\end{eqnarray*}
	\begin{enumerate}
	\item Show that $\mathbf{E}[u_i|X_i]=0$ implies that $\mathbf{E}[u_i]=0$ and $\text{Cov}(X_i,u_i)=0$.
	% {\Blue $\mathbf{E}_u[u]=\mathbf{E}_X[\mathbf{E}_u[u|X]]=E_X[0]=0$ and $\text{Cov}(X,u)=E_{X,u}[Xu]=E_X[XE_u[u|X]]=E_X[X\times 0]=0$.}

	\item The population moment conditions, $\mathbf{E}[u_i]=0$ and $\text{Cov}(X_i,u_i)=0$, give  a system of two equations for two unknowns $(\beta_0,\beta_1)$. Provide the identification result of $\beta_0$ and $\beta_1$ by solving
	\begin{align*}
	0&=\mathbf{E}[u_i]=\mathbf{E}[Y_i - ( \beta_0 + \beta_1 X_i)],\\
	0&=\text{Cov}(X_i,u_i)=\text{Cov}(X_i,Y_i - ( \beta_0 + \beta_1 X_i))
	\end{align*}
	for $\beta_0$ and $\beta_1$, by expressing $\beta_0$ and $\beta_1$ in terms of the population moments of $(X_i,Y_i)$.  Based on their sample analogues, offer an estimator of   $\beta_1$ and show that it is consistent. (HINT: the last step requires applying the Law of Large Numbers and the Continuous Mapping Theorem (CMT)) % (You can assume that the least squares estimator $\hat \beta_1$ is consistent, as it is shown in class.)

	% {\Blue Solvin $\mathbf{E}[Y_i - ( \beta_0 + \beta_1 X_i)]=0$ and $\mathbf{E}[X_i[Y_i - ( \beta_0 + \beta_1 X_i)]]=0$ for $\beta_0$ and $\beta_1$ gives $\beta_1= \frac{E[(X-E(X))Y]}{E[(X-E(X))^2]}=\frac{\text{Cov}(X,Y)}{\text{Var}(X)}$ and $\beta_0 = E[Y]-\beta_1E[X]=E[Y]-\frac{E[(X-E(X))Y]}{E[(X-E(X))^2]}E[X]$. This suggests an estimator of $\beta_1$ given by
	% \[
	% \hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)Y_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}.
	% \]
	% The numerator converges in probability to $E[(X-E(X))Y]$ because $\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)Y_i=\frac{1}{n} \sum_{i=1}^n (X_i-E(X))Y_i+(E(X)-\bar X_n)\frac{1}{n} \sum_{i=1}^n Y_i\overset{p}{\rightarrow} E[(X-E(X))Y] + 0 \times E[Y] = E[(X-E(X))Y] $ by the LLN and the CMT. The denominator converges in probability to Var$(X)$ because $\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2=\frac{1}{n}\sum_{i=1}^n (X_i-E(X))^2+\frac{1}{n}\sum_{i=1}^n (E(X)-\bar X_n)^2+2 (E(X)-\bar X_n)\times \frac{1}{n}\sum_{i=1}^n (X_i-E(X))
	% \overset{p}{\rightarrow} E[(X-E(X))^2] +\frac{1}{n}\times n \times 0 + 2\times 0 \times 0 = E[(X-E(X))^2]$ by the LLN and the CMT. Therefore,  by the CMT, 	$\hat\beta_1 \overset{p}{\rightarrow} \frac{E[(X-E(X))Y] }{E[(X-E(X))^2]}$. }


	\item  Show that $\hat \sigma^2$ is consistent for $\sigma^2$. (HINT: Replace $Y_i$ in the definition of $\hat u_i$ by its definition in terms of $X_i$'s and $u_i$'s.)

	% {\Blue Because $\hat u_i = Y_i-\hat\beta_0-\hat\beta_1 X_i=(\beta_0+\beta_1 X_i+u_i)-\hat\beta_0-\hat\beta_1 X_i=u_i +(\beta_0-\hat\beta_0) + (\beta_1-\hat \beta_1)X_i$. Therefore, using $(a+b+c)^2=a^2+b^2+c^2+2ab+2ac+2bc$ and $\frac{1}{n}\sum_{i=1}^n c=c$ for any constant $c$,
	% \begin{align*}
	% \frac{1}{n}\sum_{i=1}^n \hat u_i^2 &= \frac{1}{n}\sum_{i=1}^n u_i^2 +  (\beta_0-\hat\beta_0)^2 + (\beta_1-\hat \beta_1)^2\sum_{i=1}^n\frac{1}{n} X_i^2+ 2  (\beta_0-\hat\beta_0) \frac{1}{n}\sum_{i=1}^n u_i  \\
	% &\quad + 2  (\beta_1-\hat\beta_1) \frac{1}{n}\sum_{i=1}^n u_i X_i +  2  (\beta_0-\hat\beta_0) (\beta_0-\hat\beta_0) \frac{1}{n}\sum_{i=1}^n  X_i.
	% \end{align*}
	% Applying the LLN, $\frac{1}{n}\sum_{i=1}^n u_i^2\overset{p}{\rightarrow}\sigma^2$, $\frac{1}{n}\sum_{i=1}^n X_i^2\overset{p}{\rightarrow}E[X^2]$, $\frac{1}{n}\sum_{i=1}^n u_i\overset{p}{\rightarrow}0 $,  $ \frac{1}{n}\sum_{i=1}^n u_i X_i \overset{p}{\rightarrow}\text{Cov}(u_i,X_i)=0$,  and $\frac{1}{n}\sum_{i=1}^n  X_i\overset{p}{\rightarrow}0$. Also, $\beta_0-\hat\beta_0\overset{p}{\rightarrow}0$ and $ (\beta_1-\hat \beta_1)^2\overset{p}{\rightarrow}0$ from the previous question. Therefore, by the CMT, we have $\frac{1}{n}\sum_{i=1}^n \hat u_i^2 \overset{p}{\rightarrow}\sigma^2 +0 +0\times E[X^2]+0+0+0\times E[X] =\sigma^2 $.
	% }

	\item Show that $\sqrt{n}(\hat\beta_1-\beta_1)$ is approximately distributed according to $N(0,\sigma^2/\text{Var}(X))$  as follows.
	\begin{enumerate}
	\item Show that $\sqrt{n}(\hat\beta_1-\beta_1)$ is written as
	\[
	\sqrt{n}(\hat\beta_1-\beta) = \left(\frac{1}{n}\sum_i (X_i-\bar X_n)^2\right)^{-1} \times \left(\frac{1}{\sqrt{n}} \sum_{i=1}^n (X_i-\mathbf{E}[X])u_i + ( \mathbf{E}[X]  -\bar X_n)\frac{1}{\sqrt{n}} \sum_{i=1}^n u_i \right).
	\]
	% {\Blue By substituting $Y_i = \beta_0 + \beta_1 X_i+u_i$ to $\hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)Y_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}$, we will get $\hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}\beta_0+\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)X_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}\beta_1+\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)u_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2} = 0\times\beta_0+1\times \beta_1 +\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)u_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}$. Therefore, $\sqrt{n}(\hat\beta_1-\beta_1)=\left(\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\right)^{-1}\times \frac{1}{\sqrt{n}}\sum_{i=1}^n (X_i-\bar X_n)u_i=$\\ $\left(\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\right)^{-1}\times\left( \frac{1}{\sqrt{n}}\sum_{i=1}^n (X_i-E(X))u_i + (E(X)\bar X_n)\frac{1}{\sqrt{n}}\sum_{i=1}^n u_i \right)$.}


	\item Show that $\frac{1}{n}\sum_i (X_i-\bar X_n)^2$ converges in probability to Var$(X)$ by writing it as $\frac{1}{n}\sum_i (X_i-\bar X_n)^2=\frac{1}{n}\sum_i (X_i-\mathbf{E}[X])^2+ 2(\mathbf{E}[X]-\bar X_n)\times \frac{1}{n}\sum_i(X_i-\mathbf{E}[X])  + \frac{1}{n}\sum_i (\mathbf{E}[X]-\bar X_n)^2$ and applying the Law of Large Numbers to $\frac{1}{n}\sum_i (X_i-\mathbf{E}[X])^2$, $\frac{1}{n}\sum_i(X_i-\mathbf{E}[X])$, and $\bar X_n$ and then applying the CMT as well as the Slutsky Lemma.
	% {\Blue Applying $(a+b)^2=a^2+2ab+b^2$ with $a=X_i-E(X)$ and $b=E(X)-\bar X_n$, we have $\frac{1}{n}\sum_i (X_i-\bar X_n)^2=\frac{1}{n}\sum_i (X_i-\mathbf{E}[X])^2+ 2(\mathbf{E}[X]-\bar X_n)\times \frac{1}{n}\sum_i(X_i-\mathbf{E}[X])  + \frac{1}{n}\sum_i (\mathbf{E}[X]-\bar X_n)^2$. Applying the LLN and CMT, $\frac{1}{n}\sum_i (X_i-\bar X_n)^2\overset{p}{\rightarrow} \text{Var}(X) + 2\times 0 \times 0 + (0)^2 \times (1/n)\times n \overset{p}{\rightarrow} \text{Var}(X) $.}
	\item  Show that $( \mathbf{E}[X]  -\bar X_n)\frac{1}{\sqrt{n}} \sum_{i=1}^n u_i$ converges in probability to zero by  applying the Law of Large Numbers to $\bar X_n$ and $\frac{1}{\sqrt{n}} \sum_{i=1}^n u_i \overset{d}{\rightarrow} N(0,\sigma^2)$ by the Central Limit Theorem, and then applying the Slutsky Lemma.
	% {\Blue Because $( \mathbf{E}[X]  -\bar X_n)\overset{p}{\rightarrow} 0$ and $\frac{1}{\sqrt{n}} \sum_{i=1}^n u_i\overset{d}{\rightarrow} N(0,\sigma^2)$, $( \mathbf{E}[X]  -\bar X_n)\frac{1}{\sqrt{n}} \sum_{i=1}^n u_i\overset{p}{\rightarrow} 0\times 0$ by the CMT.}
	\item Show that $\frac{1}{\sqrt{n}} \sum_{i=1}^n (X_i-\mathbf{E}[X])u_i$ converges in distribution to $N(0,\text{Var}(X)\sigma^2)$ by applying the Central Limit Theorem and showing that $\text{Var}((X_i-\mathbf{E}[X])u_i)=\text{Var}(X)\sigma^2$ under the assumption that $\mathbf{E}[u_i|X_i]=0$  and $\mathbf{E}[u_i^2|X_i] = \sigma^2$.
	% {\Blue Because $ (X_i-\mathbf{E}[X])u_i$ are i.i.d. random variables with $E[(X_i-\mathbf{E}[X])u_i]=0$ and Var$( (X_i-\mathbf{E}[X])u_i)$ is finite, we may apply the CLT to obtain $\frac{1}{\sqrt{n}} \sum_{i=1}^n (X_i-\mathbf{E}[X])u_i \overset{d}{\rightarrow} N(0,\text{Var}( (X_i-\mathbf{E}[X])u_i))$. Furthermore, $\text{Var}( (X_i-\mathbf{E}[X])u_i))=E[ (X_i-\mathbf{E}[X])^2u_i^2 ] - \{ E[ (X_i-\mathbf{E}[X])u_i]\}^2$. By the Law of Iterated Expectation, $E[ (X_i-\mathbf{E}[X])^2u^2 ]=E_X[(X-\mathbf{E}[X])^2E_u[u^2|X]] =E_X[(X-\mathbf{E}[X])^2\sigma^2] = \sigma^2E_X[(X-\mathbf{E}[X])^2]=\sigma^2 \text{Var}(X)$ and $ \{ E[ (X_i-\mathbf{E}[X])u_i]\}^2= \{0\}^2$ because $E[ (X_i-\mathbf{E}[X])u_i]=E_X[(X-E(X))E_u[u|X]]=0$. Therefore, $\text{Var}( (X_i-\mathbf{E}[X])u_i))=\sigma^2 \text{Var}(X)$.}
	\item Apply the CMT and the Slutsky Lemma with the results from 1-4 above to show $\sqrt{n}(\hat\beta_1-\beta)\overset{d}{\rightarrow} N(0,\sigma^2/\text{Var}(X))$. 
	% {\Blue Combining the results of 1-4 and applying the Slutsky lemma and the CMT, we get $\sqrt{n}(\hat\beta_1-\beta)\overset{d}{\rightarrow} \text{Var}(X)^{-1} N( 0, \sigma^2 \text{Var}(X)) = N(0,\sigma^2/\text{Var}(X))$.}
	\end{enumerate}

	\item Briefly discuss what will happen to the asymptotic distribution of $\sqrt{n}(\hat\beta_1-\beta)$ when $\text{Var}(X)\rightarrow 0$. 
	% {\Blue when $\text{Var}(X)\rightarrow 0$, the asymptotic variance of $\sqrt{n}(\hat\beta_1-\beta)$ diverges to $\infty$ and the distribution of $\sqrt{n}(\hat\beta_1-\beta)$ becomes dispersed. This is because, as $\text{Var}(X)\rightarrow 0$, $\beta_0$ and $\beta_1$ are not separately identified in population and, therefore, we are not able to consistently estimate $\beta_1$.}

%	\item Suppose now that  the variance of $u_i$ depends on the value of $X_i$ such that $\mathbf{E}[u_i^2|X_i] =  \sigma^2 \exp(X_i)$ holds  instead of $\mathbf{E}[u_i^2|X_i] =  \sigma^2$. In view of the proof in (d), what is the asymptotic distribution of $\sqrt{n}(\hat\beta_1-\beta)$?


\end{enumerate}


	\item Suppose that we are considering the following univariate regression
	model:
	\begin{equation*}
	Y_{i}=\beta_0 + \beta_1 X_i+u_{i},
	\end{equation*}%
	where we have $\mathbf{E}[u_{i}|X_{i}]=0$ and $Var(X_i) >0$. Suppose that while we observe $X_i$, we do not observe $Y_i$. Instead, we observe $\tilde Y_i$ which is defined to be
	\begin{eqnarray*}
		\tilde Y_i = Y_i + e_i,
	\end{eqnarray*}
where $e_i$ is a measurement error. Let us assume that
\begin{eqnarray*}
	Cov(e_i,u_i) = 0, \text{ and } Cov(e_i,X_i) = 0.
\end{eqnarray*}
In this situation, we observe an i.i.d. sample of $\{(\tilde Y_i,X_i)\}_{i=1}^n$.
\bigskip
\begin{enumerate}
\item Show that $\beta_1$ is identified, but $\beta_0$ is not.
% {\Blue
% Because $\mathbf{E}[u_{i}|X_{i}]=0$ and  $Cov(e_i,X_i) = 0$  imply $Cov(u_i+e_i,X_i)=Cov(u_i,X_i)+Cov(e_i,X_i)=0$.

% Substituting $Y_i =\tilde Y_i - e_i$ to $Y_{i}=\beta_0 + \beta_1 X_i+u_{i}$ gives
% \[
% \tilde Y_i =\beta_0 + \beta_1 X_i+(u_{i}+e_i).
% \]
% Subtracting $E[\tilde Y] =\beta_0 + \beta_1 E[X]+E[(u+e)]$ from the above equation and multiplying it by $(X_i-E(X))$, we have
% \[
% (\tilde Y_i -E[\tilde Y])  (X_i-E[X])=  \beta_1 (X_i-E[X])^2+ ((u_{i}+e_i)-E[(u+e)]) (X_i-E[X]).
% \]
% Taking the expectation of the above equation and using $Cov(u_i+e_i,X_i)=0$ give
% \[
% \beta_1 = \frac{\text{Cov}(\tilde Y,X) }{\text{Var}(X)}
% \]
% and $\beta_0=E[\tilde Y]  -  \frac{\text{Cov}(\tilde Y,X) }{\text{Var}(X)}E[X]-E[e]$. Because $E[e]$ is not assumed to be zero and unknown, we cannot identify $\beta_0$ separately from the value of $E[e]$.  }
\bigskip

\item  Show that the least squares estimator of $\beta_1$ obtained by regressing $\tilde Y_i$ on $X_i$ is consistent.\bigskip

% {\Blue$\hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)\tilde{Y}_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}$, we will get $\hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}\beta_0+\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)X_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}\beta_1+\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)u_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2} = 0\times\beta_0+1\times \beta_1 +\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)(u_i + e_i)}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}$.

% Apply the LLN, $\frac{1}{n} \sum_{i=1}^n (X_i- \bar X_n)(u_i + e_i) \overset{p}{\rightarrow} \text{Cov}(X,u+e) = 0$,
% $\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2 \overset{p}{\rightarrow} \text{Var}(X)$.
% Therefore, $\hat{\beta}_1 \overset{p}{\rightarrow} \beta.$
% }

\item  Suppose that $\mathbf{E}[u_i^2|X_i] = \sigma_u^2>0$ and $\mathbf{E}[e_i^2|X_i] = \sigma_e^2>0$, $\mathbf{E}[e_i|X_i]=0$ and $\mathbf{E}[u_i e_i|X_i]=0$. Find the asymptotic variance $\sigma^2_\beta$ of the least squares estimator $\hat \beta_1$. In other words, find the formula for $\sigma^2_\beta$ such that
\begin{eqnarray*}
	\sqrt{n}(\hat \beta_1 -\beta_1) \rightarrow_d N(0,\sigma_\beta^2).
\end{eqnarray*}
(It suffices to sketch the derivation of the asymptotic normality and the asymptotic variance using the similar arguments shown in class.) Does the presence of measurement error influence the asymptotic variance $\sigma_\beta^2$? Does it increase with the variance of the measurement error? Explain your answer.
\end{enumerate}
\bigskip

	% {\Blue By substituting $\tilde{Y}_i = \beta_0 + \beta_1 X_i+u_i + e_i$ to $\hat\beta_1 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)\tilde{Y}_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2}$, we will get $\hat\beta_1 = \beta_1+\frac{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)(u_i + e_i)}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2} $.

	% Therefore, $\sqrt{n}(\hat\beta_1-\beta_1)=\left(\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\right)^{-1}\times \frac{1}{\sqrt{n}}\sum_{i=1}^n (X_i-\bar X_n)(u_i + e_i) =$\\ $\left(\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\right)^{-1}\times\left( \frac{1}{\sqrt{n}}\sum_{i=1}^n (X_i-\bar X_n)(u_i + e_i) \right) \overset{d}{\rightarrow} \text{Var}(X_i)^{-1} N(0, (\sigma_u^2 + \sigma_e^2) \text{Var}(X_i))$, because $\text{Cov}(e_i,u_i) = 0,\text{Cov}(e_i,X_i) = 0, \mathbf{E}[u_i | X_i] = 0$. Therefore $\sigma_{\beta}^2 =  (\sigma_u^2 + \sigma_e^2) / \text{Var}(X_i).$ 

	% The measurement error increase the asymptotic variance of $\sigma_{\beta}^2$, because $ \sigma_u^2 + \sigma_e^2 > \sigma_u^2 $.}


 \item Suppose that a researcher is interested in analyzing the effect of age on the log of earnings. Consider the following polynomial regression model with the interaction term:
 \begin{eqnarray*}
 	Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i D_i + \beta_3 X_i^2 + \beta_4 D_i + u_i,
 \end{eqnarray*}
 where we assume that $\mathbf{E}[u_i|X_i,D_i] = 0$. Here $X_i$ denotes the $i$-th person's age and $D_i$ denotes the gender variable, $D_i=1$ representing that the $i$-th person is female, and $Y_i$ the $i$-th person's log of earnings. Suppose that we are using the regression model as a causal model.


\begin{enumerate}
\item Provide a system of five equations in population whose solution gives the identification of $\beta_0$, $\beta_1,\beta_2,\beta_3$ and $\beta_4$.
\bigskip

% {\Blue With $\mathbf{E}[u_i | X_i,D_i] = 0$, we have that
% \begin{eqnarray*}
% 	\text{cov}(Y_i,X_i | D_i = 1) =   (\beta_0 + \beta_4) + (\beta_1 + \beta_2) \text{Var}(X_i | D_i = 1) +   \beta_3 \text{cov}(X_i^2,X_i| D_i = 1) \\
% 	\text{cov}(Y_i,X_i | D_i = 0) =   \beta_0  + \beta_1  \text{Var}(X_i | D_i = 1) +   \beta_3 \text{cov}(X_i^2,X_i| D_i = 0)  \\
% 	\text{cov}(Y_i,X_i^2 ) =   \beta_0  + \beta_1 \text{Cov}(X_i^2,X_i) + \beta_2 \text{Cov}(X_iD_i, X_i^2) + \beta_3 \text{Var}(X_i^2) + \beta_4 D_i \\	
% 	 \mathbf{E}[(Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i D_i - \beta_3 X_i^2 - \beta_4 D_i) |D_i = 1 ] = 0 \\
% 	 \mathbf{E}[(Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i D_i - \beta_3 X_i^2 - \beta_4 D_i) |D_i = 0 ] = 0 \\	
%  % \mathbf{E}[(Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i D_i - \beta_3 X_i^2 - \beta_4 D_i) ] = 0 .
% \end{eqnarray*}
% Solve the equation system will give us the identification of $\beta_0,\beta_1,\beta_2,\beta_3,\beta_4$.
% }
\item  Compute $E[Y_i|X_i,D_i=1]-E[Y_i|X_i,D_i=0]$ and express $\beta_2$ in terms of $\partial (E[Y_i|X_i,D_i=1]-E[Y_i|X_i,D_i=0])/\partial X_i$.
Suppose that $\beta_2$ is estimated to be significantly positive. Give a verbal interpretation of this result.
%\medskip

% {\Blue
% $\mathbf{E}[Y_i | X_i, D_i = 1] = (\beta_0 + \beta_4) + (\beta_1 + \beta_2) X_i + \beta_3 X_i^2$, $\mathbf{E}[Y_i | X_i, D_i = 0] = \beta_0  + \beta_1  X_i + \beta_3 X_i^2$. Therefore $\mathbf{E}[Y_i | X_i, D_i = 1] - \mathbf{E}[Y_i | X_i, D_i = 0] = \beta_0 +  \beta_2 X_i$.
% $\partial (E[Y_i|X_i,D_i=1]-E[Y_i|X_i,D_i=0])/\partial X_i = \beta_2$.
% If $\beta$ is significantly positive, it means the female will have higher growth of earning given the increase of age.
% }

\item Find the formula for the average partial effect of changing $X_i$ on $Y_i$, and explain how you would estimate it.

% {\Blue The average partial effect of changing $X_i$ on $Y_i$ is the expectation of the local partial effect.
% $\mathbf{E}_{X} [\partial \mathbf{E}[Y_i | X_i] / \partial X_i] = \beta_1 + 2 * \beta_3 \mathbf{E}_{X}[X_i] + \beta_2 \mathbf{E}_{X}[\mathbf{E}[ D_i|X_i]] = \beta_1 + 2 * \beta_3 \mathbf{E}[X_i] + \beta_2 \mathbf{E}[ D_i]$.

% The sample analogue estimator of the average partial is:

% $\widehat{\mathbf{E}_{X} [\partial \mathbf{E}[Y_i | X_i] / \partial X_i]} = \hat{\beta}_1 + 2 * \hat{\beta}_3 \bar{X}_n + \beta_2 \bar{D}_n,$
% where $\hat{\beta}_1,\hat{\beta}_2,\hat{\beta}_3$ are estimated linear regression coefficients, and $\bar{X}_n,\bar{D}_n$ are sample mean of $X_i$ and $D_i$.
% }

    \end{enumerate}

    \end{enumerate}

\bigskip



\bibliographystyle{apalike}
\bibliography{library}
\end{document}
